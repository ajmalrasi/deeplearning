{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define neural network layers and units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(layer_dims):\n",
    "    params = {}\n",
    "    for l in range(1,len(layer_dims)):\n",
    "        params[\"W\"+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/(layer_dims[l-1]+layer_dims[l]))\n",
    "        params[\"b\"+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax(Z, axis=None):\n",
    "    cache = Z\n",
    "    Z = Z - Z.max(axis=axis, keepdims=True)\n",
    "    y = np.exp(Z)\n",
    "    return y / np.sum(y, axis=axis, keepdims=True), cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"relu\":\n",
    "        Z, linear_cache  = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    elif activation ==\"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, params):\n",
    "    caches = list()\n",
    "    A = X\n",
    "    L = len(params) // 2 # no of paramaters divided by 2 w1 and b1 for no of layers\n",
    "    for l in range(1, L):\n",
    "        W = params[\"W\"+str(l)]\n",
    "        b = params[\"b\"+str(l)]\n",
    "        A, cache = linear_activation_forward(A, W, b, \"relu\")\n",
    "        caches.append(cache)\n",
    "    #final layer\n",
    "    W = params[\"W\"+str(L)]\n",
    "    b = params[\"b\"+str(L)]\n",
    "    AL, cache = linear_activation_forward(A, W, b, \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    #logarithimic cost\n",
    "    cost = -(1/m)*np.sum((Y * np.log(AL) ) + ((1 - Y)*np.log(1-AL)))\n",
    "    #cost = -(1/m)*np.sum( Y * np.log(AL) )\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "#     dAL = -(1/1)*(Y / AL) + ((1 - Y) / (1 - AL))\n",
    "#     dAL = -(1/1)*(Y / AL) + ((1 - Y) / (1 - AL))\n",
    "    dAL = -(Y * (1/AL) ) + ( (1 - Y) * (1/(1-AL)) )\n",
    "    return dAL\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def softmax_backward(dA, cache):\n",
    "    z2e = np.exp(cache)\n",
    "    temp = np.zeros((z2e.shape))\n",
    "    for i in range(z2e.shape[0]):\n",
    "        mask = np.ones(z2e.shape, dtype=bool)\n",
    "        mask[i] = 0\n",
    "        temp[i] =  (z2e[i] * np.sum(z2e[mask])) / (np.sum(z2e) * 2)\n",
    "    return temp * dA\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(AL, Y, caches):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = d_cost(AL, Y)\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * grads[\"dW\" + str(l+1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * grads[\"db\" + str(l+1)])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(AL, Y, threshold = 0.5):\n",
    "    AL[AL <=threshold] = 0\n",
    "    AL[AL >threshold] = 1\n",
    "    correct = (Y == AL)\n",
    "    return (np.sum(correct) /correct.shape[1]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, layers_dims, learning_rate = 0.001, num_iterations = 1000, print_cost=True):\n",
    "\n",
    "    costs = [] # keep track of cost\n",
    "    params = init_parameters(layer_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = forward_prop(X, params)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads =  back_prop(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        params = update_parameters(params, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            acc = accuracy(AL, Y_train)\n",
    "            print(\"Cost after iteration {0}: {1:.4f} - acc: {2:.2f}\".format(i,cost,acc))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if cost < 0.0001:\n",
    "            return params, AL\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return params, AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, Y, split=80):\n",
    "    dataset_size = X.shape[1]\n",
    "    m_train = math.floor((dataset_size * split)/100)\n",
    "    X_train = X[:,0:m_train]\n",
    "    X_test = X[:,m_train-1:-1]\n",
    "    Y_train = Y[:,0:m_train]\n",
    "    Y_test = Y[:,m_train-1:-1]\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"dataset/skin.csv\")\n",
    "\n",
    "raw_data.values[:,-1] = (raw_data.values[np.arange(0,raw_data.values.shape[0]),-1] == 1 ) + 0\n",
    "shuffled_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = shuffled_data.values[:,0:-1].T #Transpose to make it in appropriat format\n",
    "Y = np.array([shuffled_data.values[:,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train, Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = train_test_split(X, Y, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_1 = params # [n_x, 16, 16, n_y] , num_iterations = 10000, learning_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = params_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Normalize and etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255.\n",
    "X_test = X_test/255.\n",
    "m_train = X_train.shape[1]\n",
    "m_test = X_test.shape[1]\n",
    "n_x = X_train.shape[0]\n",
    "n_y = Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network and Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6707 - acc: 76.33\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # ## define the network here\n",
    "layer_dims = [n_x, 16, 16, n_y]\n",
    "\n",
    "params, AL = train(X_train, Y_train, layer_dims, print_cost=True, num_iterations = 1000, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, caches = forward_prop(X_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.36734693877551"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(AL, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, caches = forward_prop(X_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.24725604650118"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(AL, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
