{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dicts = pickle.load(fo, encoding='bytes')\n",
    "        \n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y_t, n_y):\n",
    "    Y_hot = np.zeros((n_y,Y_t.shape[1]))\n",
    "    Y_hot[Y_t, np.arange(Y_t.shape[1])] = 1\n",
    "    return Y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_AL(AL):\n",
    "    AL_argmax = np.argmax(AL,axis=0)\n",
    "    hot = np.zeros((AL.shape),dtype=np.int)\n",
    "    hot[AL_argmax,np.arange(AL.shape[1])] = 1\n",
    "    return hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save():\n",
    "    with open('models/params_16_8.p', 'wb') as f:\n",
    "        pickle.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define neural network layers and units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(layer_dims, xavier=False):\n",
    "    params = {}\n",
    "    for l in range(1,len(layer_dims)):\n",
    "        if xavier:\n",
    "            params[\"W\"+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/(layer_dims[l-1]))\n",
    "        else:\n",
    "            params[\"W\"+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        params[\"b\"+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax(Z, axis=0):\n",
    "    cache = Z\n",
    "    Z = Z - Z.max(axis=axis, keepdims=True)\n",
    "    y = np.exp(Z)\n",
    "    return y / np.sum(y,axis=axis), cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"relu\":\n",
    "        Z, linear_cache  = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    elif activation ==\"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, params):\n",
    "    caches = list()\n",
    "    A = X\n",
    "    L = len(params) // 2 # no of paramaters divided by 2 w1 and b1 for no of layers\n",
    "    for l in range(1, L):\n",
    "        W = params[\"W\"+str(l)]\n",
    "        b = params[\"b\"+str(l)]\n",
    "        A, cache = linear_activation_forward(A, W, b, \"relu\")\n",
    "        caches.append(cache)\n",
    "    #final layer\n",
    "    W = params[\"W\"+str(L)]\n",
    "    b = params[\"b\"+str(L)]\n",
    "    AL, cache = linear_activation_forward(A, W, b, \"softmax\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, lambd, caches):\n",
    "    m = Y.shape[1]\n",
    "    weight_sum = 0\n",
    "    #logarithimic cost\n",
    "    \n",
    "    logprobs = -np.multiply(np.log(AL),Y) + np.multiply(-np.log(1 - AL), 1 - Y)\n",
    "    non_reg_cost = 1./m * np.nansum(logprobs)\n",
    "    \n",
    "    for i in range(0, len(caches)):\n",
    "        linear_cache, _ = caches[i]\n",
    "        _, W ,_  = linear_cache\n",
    "        weight_sum += np.sum(np.square(W))\n",
    "            \n",
    "    L2 = (1/m) * (lambd /2) * weight_sum\n",
    "\n",
    "    return non_reg_cost + L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    dAL = -(Y * (1/AL) ) + ( (1 - Y) * (1/(1-AL)) )\n",
    "    return dAL\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def relu_gradient(dA, Z):\n",
    "    A, Z = relu(Z)\n",
    "    dZ = np.multiply(dA, np.int64(A > 0))\n",
    "    return dZ\n",
    "\n",
    "def softmax_backward(dA, cache):\n",
    "    z2e = np.exp(cache)\n",
    "    temp = np.zeros((z2e.shape))\n",
    "    for i in range(z2e.shape[0]):\n",
    "        mask = np.ones(z2e.shape, dtype=bool)\n",
    "        mask[i] = 0\n",
    "        temp[i] =  (z2e[i] * np.sum(z2e[mask])) / (np.sum(z2e) * 2)\n",
    "    return temp * dA\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache, lambd):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T) + (lambd / m) * W\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, lambd, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_gradient(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache, lambd)\n",
    "    elif activation == \"softmax\":\n",
    "        #dZ = softmax_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dA, linear_cache, lambd) #already calculated\n",
    "    elif activation == \"sigmoid\":\n",
    "        #dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dA, linear_cache, lambd) #already calculated\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(Al, Y, caches, lambd):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = Al.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    #dAL = d_cost(AL, Y)\n",
    "    dZ = Al - Y\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dZ, current_cache, lambd, activation=\"softmax\")\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, lambd, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * grads[\"dW\" + str(l+1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * grads[\"db\" + str(l+1)])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(AL, Y, threshold = 0.5):\n",
    "    Y_pred = one_hot_AL(AL)\n",
    "    correct = (Y != Y_pred)\n",
    "    return 100 - ( (np.sum(correct/2) /correct.shape[1]) * 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, X_test, Y_test, layers_dims, lr = 0.005, num_iterations = 1000, print_cost=True, lambd = 5, re_train=False, param = None):\n",
    "\n",
    "    costs = [] # keep track of cost\n",
    "    if re_train:\n",
    "        params = param\n",
    "    else:\n",
    "        params = init_parameters(layer_dims)\n",
    "      \n",
    "    m = Y.shape[1]\n",
    "    global AL\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SOFTMAX.\n",
    "        AL, caches = forward_prop(X, params)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y, lambd, caches)\n",
    "        \n",
    "        # Backward propagation.\n",
    "        grads =  back_prop(AL, Y, caches, lambd)\n",
    " \n",
    "        # Update parameters.\n",
    "        params = update_parameters(params, grads, lr)\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            train_acc = accuracy(AL, Y_train)\n",
    "            AL_test, _ = forward_prop(X_test, params)\n",
    "            test_acc = accuracy(AL_test, Y_test)\n",
    "            print(\"Cost after iteration {0}: {1:.4f} - train acc: {2:.2f} - test acc: {3:.2f}\".format(i,cost,train_acc,test_acc))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "        #Save the weights every 1000 iterations\n",
    "        if i % 1000 == 0:\n",
    "            save()\n",
    "            \n",
    "    #Save after training\n",
    "    with open('models/params_end_reg_16_8.p', 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(lr))\n",
    "    plt.show()\n",
    "    \n",
    "    return params, AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, Y, split = 80):\n",
    "    dataset_size = X.shape[1]\n",
    "    m_train = math.floor((dataset_size * split)/100)\n",
    "    X_train = X[:,0:m_train]\n",
    "    X_test = X[:,m_train-1:-1]\n",
    "    Y_train = Y[:,0:m_train]\n",
    "    Y_test = Y[:,m_train-1:-1]\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = unpickle(\"dataset/data_batch_1\")\n",
    "labels  = unpickle(\"dataset/batches.meta\")[b'label_names']\n",
    "X = data[b'data']\n",
    "Y = np.array(data[b'labels'])\n",
    "X = data[b'data'].T\n",
    "Y = Y.reshape((1,Y.shape[0]))\n",
    "dataset_size = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:,0:1000]\n",
    "Y = Y[:,0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_y = len(labels)\n",
    "Y = one_hot(Y, n_y)\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = train_test_split(X, Y, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = X_train/255.\n",
    "X_test_norm = X_test/255.\n",
    "m_train = X_train.shape[1]\n",
    "m_test = X_test.shape[1]\n",
    "n_x = X_train.shape[0]\n",
    "n_y = Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/params_end_reg_16_8.p', 'rb') as f:\n",
    "    params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.2925 - train acc: 78.62 - test acc: 23.50\n",
      "Cost after iteration 100: 1.2900 - train acc: 78.62 - test acc: 23.50\n",
      "Cost after iteration 200: 1.2874 - train acc: 78.62 - test acc: 23.50\n",
      "Cost after iteration 300: 1.2848 - train acc: 78.62 - test acc: 23.50\n",
      "Cost after iteration 400: 1.2823 - train acc: 78.75 - test acc: 23.50\n",
      "Cost after iteration 500: 1.2797 - train acc: 78.75 - test acc: 23.50\n",
      "Cost after iteration 600: 1.2772 - train acc: 78.88 - test acc: 23.50\n",
      "Cost after iteration 700: 1.2749 - train acc: 79.12 - test acc: 23.50\n",
      "Cost after iteration 800: 1.2720 - train acc: 79.12 - test acc: 23.50\n",
      "Cost after iteration 900: 1.2697 - train acc: 79.38 - test acc: 23.50\n",
      "Cost after iteration 1000: 1.2669 - train acc: 79.38 - test acc: 23.50\n",
      "Cost after iteration 1100: 1.2643 - train acc: 79.38 - test acc: 23.50\n",
      "Cost after iteration 1200: 1.2619 - train acc: 79.50 - test acc: 23.50\n",
      "Cost after iteration 1300: 1.2593 - train acc: 79.50 - test acc: 23.50\n",
      "Cost after iteration 1400: 1.2566 - train acc: 79.62 - test acc: 24.00\n",
      "Cost after iteration 1500: 1.2539 - train acc: 79.62 - test acc: 24.00\n",
      "Cost after iteration 1600: 1.2514 - train acc: 79.62 - test acc: 24.00\n",
      "Cost after iteration 1700: 1.2490 - train acc: 79.75 - test acc: 24.50\n",
      "Cost after iteration 1800: 1.2463 - train acc: 79.75 - test acc: 24.50\n",
      "Cost after iteration 1900: 1.2438 - train acc: 79.75 - test acc: 24.50\n",
      "Cost after iteration 2000: 1.2411 - train acc: 79.75 - test acc: 24.50\n",
      "Cost after iteration 2100: 1.2387 - train acc: 79.75 - test acc: 24.50\n",
      "Cost after iteration 2200: 1.2359 - train acc: 79.75 - test acc: 24.00\n",
      "Cost after iteration 2300: 1.2335 - train acc: 79.88 - test acc: 24.50\n",
      "Cost after iteration 2400: 1.2308 - train acc: 79.75 - test acc: 24.50\n",
      "Cost after iteration 2500: 1.2283 - train acc: 80.00 - test acc: 24.00\n",
      "Cost after iteration 2600: 1.2258 - train acc: 80.00 - test acc: 24.00\n",
      "Cost after iteration 2700: 1.2233 - train acc: 80.00 - test acc: 24.00\n",
      "Cost after iteration 2800: 1.2210 - train acc: 80.00 - test acc: 24.00\n",
      "Cost after iteration 2900: 1.2181 - train acc: 80.12 - test acc: 24.00\n",
      "Cost after iteration 3000: 1.2159 - train acc: 80.12 - test acc: 24.00\n",
      "Cost after iteration 3100: 1.2132 - train acc: 80.12 - test acc: 24.00\n",
      "Cost after iteration 3200: 1.2106 - train acc: 80.38 - test acc: 24.00\n",
      "Cost after iteration 3300: 1.2081 - train acc: 80.25 - test acc: 24.00\n",
      "Cost after iteration 3400: 1.2056 - train acc: 80.25 - test acc: 24.00\n",
      "Cost after iteration 3500: 1.2031 - train acc: 80.25 - test acc: 25.00\n",
      "Cost after iteration 3600: 1.2005 - train acc: 80.25 - test acc: 24.50\n",
      "Cost after iteration 3700: 1.1980 - train acc: 80.38 - test acc: 25.00\n",
      "Cost after iteration 3800: 1.1954 - train acc: 80.62 - test acc: 25.00\n",
      "Cost after iteration 3900: 1.1932 - train acc: 80.62 - test acc: 25.00\n",
      "Cost after iteration 4000: 1.1905 - train acc: 80.62 - test acc: 25.00\n",
      "Cost after iteration 4100: 1.1878 - train acc: 80.62 - test acc: 25.00\n",
      "Cost after iteration 4200: 1.1854 - train acc: 80.75 - test acc: 25.00\n",
      "Cost after iteration 4300: 1.1826 - train acc: 80.88 - test acc: 25.00\n",
      "Cost after iteration 4400: 1.1800 - train acc: 81.00 - test acc: 25.00\n",
      "Cost after iteration 4500: 1.1774 - train acc: 81.00 - test acc: 25.00\n",
      "Cost after iteration 4600: 1.1749 - train acc: 81.00 - test acc: 25.00\n",
      "Cost after iteration 4700: 1.1723 - train acc: 81.12 - test acc: 25.00\n",
      "Cost after iteration 4800: 1.1696 - train acc: 81.12 - test acc: 25.00\n",
      "Cost after iteration 4900: 1.1666 - train acc: 81.25 - test acc: 25.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## define the network here\n",
    "layer_dims = [n_x, 16, 8, n_y]\n",
    "\n",
    "params, AL = train(X_train_norm, Y_train, X_test_norm, Y_test, layer_dims,num_iterations = 5000,lr = 0.001, lambd = 0.2, re_train = True, param = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
